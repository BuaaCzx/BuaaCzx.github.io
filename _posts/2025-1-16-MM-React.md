---
layout: post
title: 多模态agent文献略读（持续更新）
categories: [文献阅读, video agent]
tags: [LLM, 学习笔记]
slug: multimodal-fast-reading
description: 一些相关工作，但是可能没那么有用或者是不太新，粗浅看看introduction和方法，记录在这里。
---

## Large Multimodal Agents: A Survey

https://arxiv.org/abs/2402.15116

综述，介绍了 LMAs 的核心组成、分类、多智能体协作、评估、应用、总结与未来展望

感觉大部分讲的是视觉，以及模拟人类行为相关的，和生成式智能体比较相关的不多。

在应用部分，有一个小块提到了视觉内容生成和编辑。

![image-20250115162446272](./../images/2025-1-16-MM-React/image-20250115162446272.png)

-  [Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing.](https://arxiv.org/pdf/2311.00571)
-  [Chatvideo: A tracklet-centric multimodal and versatile video understanding system](https://arxiv.org/abs/2304.14407)

-  [Mm-react: Prompting chatgpt for multimodal reasoning and action.](https://arxiv.org/abs/2303.11381)

看上去只有第一个是生成视频，后两个是视频内容理解，是视频生成文本，后续再看原文。

##MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action

https://arxiv.org/abs/2303.11381

灵活的把视觉模型和语言模型结合起来，以解决复杂的视觉理解问题。

看起来是一个早期的工作，借助视觉模型把多模态信息转换成文本，让文本模型实现多模态理解。

![image-20250116111628443](./../images/2025-1-16-MM-React/image-20250116111628443.png)

每当 ChatGPT 需要图像或视频中的内容时，就寻求视觉模型的帮助。在 prompt 里添加每个模型的使用说明，并设置了口令，使得可以通过正则表达式来匹配并调用视觉模型。

## ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System
https://arxiv.org/abs/2304.14407

先全面解析视频，在于用户交互时适当查询有用信息。把视频中对象的运动轨迹作为视频的基本单位，把对象的种类、外观、运动、轨迹存在数据库里，要用的时候就查出来给 LLM 参考做响应。

![image-20250116133607782](./../images/2025-1-16-MM-React/image-20250116133607782.png)

这项工作和上面的工作，还有一个叫 Visual CahtGPT 的工作，都是为了把 ChatGPT 和现有的视觉模型连接起来（而不是训练新模型），使他们交互，从而实现视觉聊天。

## LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing