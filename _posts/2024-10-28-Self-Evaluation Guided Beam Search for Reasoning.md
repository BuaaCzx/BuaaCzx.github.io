---
layout: post
title: Self-Evaluation Guided Beam Search for Reasoning
categories: [文献阅读]
tags: [LLM, 学习笔记]
slug: Self-Evaluation-Guided-Beam-Search-for-Reasoning
---

让模型在推理的每一步进行自我评估和校正，结合随机性探索以提高多步骤推理的准确性和稳定性。

https://arxiv.org/abs/2305.00633

初步调研，没细看论文。

## Abstract

将问题分解为中间步骤在大型语言模型（LLM）的推理中表现出色。然而，**推理链的增长会带来不确定性和错误积累**，从而难以获得准确的最终结果。为了解决多步推理中的不确定性挑战，我们引入了一种 **逐步自我评估机制**，以指导和校准LLM的推理过程。

这种自我评估机制能够定位逻辑错误，导致更高的推理一致性和稳定性。

代码公开在 https://guideddecoding.github.io/。

## ChatGPT 通俗易懂版解读

当人工智能模型在解决复杂问题时，它通常需要一步一步地推理，但问题在于，这个过程如果出错，每一步的错误都会累积，使得最终结果不准确。为了解决这个问题，这篇论文提出了一种让模型在每一步都“自我检查”的新方法。

### 方法核心思想
假设模型在做一道数学题或逻辑推理题，它每完成一步，都停下来问自己：“这一步做对了吗？” 然后，它会给自己的答案一个评分，就像是自己当自己的小老师。如果它觉得这一步做得不够好，就会尝试重新调整。

### 如何实现？
1. **逐步检查**：每一步生成的内容都经过模型自己的评估打分，类似于学生在考试中每解一步题都会回头检查自己的解题步骤。
2. **智能筛选**：模型会尝试多种解题路径，每一步不仅看得分高不高，还会随机挑选一些可能性进行尝试，以找到更好的解题方法。
3. **平衡随机性与准确性**：为了避免总是选择“看起来最保险的”路径，方法中加了一点随机性，这样模型有机会探索新的、更好的解题路径。

### 举个例子
假设模型在回答“玛丽和哈罗德共卖了88,000张唱片，如果玛丽卖的数量是哈罗德的10倍，哈罗德卖了多少张？”模型在每一步计算中都会问自己：“我这步的计算正确吗？” 如果不对，它会及时调整，避免把错误带到最后一步。

### 为什么有效？
这种方法相当于让模型“边做题，边检查”，避免了在早期步骤中的错误一直被带到最后。这样，不仅提高了准确性，还让模型在遇到需要复杂推理的题目时更加稳定和灵活。

所以，这个方法就像是让模型学会在解题中“反思”和“自纠”，从而能更好地解决问题。
